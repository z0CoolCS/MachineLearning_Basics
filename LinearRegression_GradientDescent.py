# -*- coding: utf-8 -*-
"""Homework1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nopE8uW7YJIq93t1h8uDxvvB9BLjOEAJ
"""

import numpy as np
import matplotlib.pyplot as plt

def h(x, w):
  return x @ w

"""**LOSS FUNCTION Minimum Square Error (MSE)**"""

def error_mse(x, y, w):
  return sum((h(x, w) - y) ** 2) / (2*len(y))

def loss_function_mse(x, y, w):
  return np.sum((h(x, w) - y) * x, axis = 0, keepdims= True).T / len(y)

"""**LOSS FUNCTION Minimum Absolute Error (MAE)**"""

def error_mae(x, y, w):
  return sum(np.abs(h(x, w) - y)) / len(y)

def loss_function_mae(x, y, w):
  diff = (h(x, w) - y)
  return np.sum((diff / np.abs(diff)) * x, axis = 0, keepdims= True).T / len(y)

"""**LOSS FUNCTION WITH REGULAIRZATION RIDGE**"""

def regularization_ridge_error(error_function, x, y, w):
  w_0 = np.copy(w)
  w_0[0] = 0
  return error_function(x, y, w) + (lambda_v * sum(w_0*w_0)) /  (2*len(y))

def ridge_regression(w):
  with_w0 = lambda_v * w 
  with_w0[0][0] = 0
  return with_w0

def lost_function_ridge(loss_function, x, y, w):
  return loss_function(x, y, w) + (ridge_regression(w))/ len(y)

"""**LOSS FUNCTION WITH REGULARIZATION LASSO**"""

def regularization_lasso_error(error_function, x, y, w):
  w_0 = np.copy(w)
  w_0[0] = 0
  return error_function(x, y, w) + (lambda_v * np.abs(w_0))/ (2*len(y))

def lasso_regression(w):
  with_w0 = lambda_v * w / np.abs(w)
  with_w0[0][0] = 0
  return with_w0

def lost_function_lasso(loss_function, x, y, w):
  return loss_function(x, y, w) + lasso_regression(w)/ len(y)

"""**Generate DataSet**"""

x = np.arange(0, 1, .01)
y = np.array([np.sin(2*i*np.pi)+np.random.normal(0,0.2)  for i in x])
y.shape = (len(x), 1) #(10, 1) nrows = 10, ncols = 1
ones = np.ones(len(x))
x = np.c_[ones, x]
plt.scatter(x[:,1], y)
plt.show()

"""**Generate Train and Test DataSet**"""

def getSetTraining(x, y):
  index = np.arange(0, len(y), step = 1)
  index_limit = int(.75*len(index))
  np.random.shuffle(index)
  index_train, index_test = index[:index_limit], index[index_limit:]
  index_train = np.sort(index_train)
  index_test = np.sort(index_test)
  return x[index_train,:], x[index_test,:],y[index_train,:], y[index_test,:]
x_train, x_test, y_train, y_test = getSetTraining(x, y)

def rescaled(x_p):
  dim = len(w)
  i = x_train.shape[1] - 1
  xcopy = np.copy(x_p)
  while (dim > i + 1):
      x_tmp = xcopy[:,i] * xcopy[:,1]
      xcopy = np.c_[xcopy, x_tmp]
      i += 1
  return xcopy

"""**Batch Gradient Descent**"""

def batch_gradient_descent(loss_function, x, y, w):
  return - alpha * loss_function(x, y, w)

"""**Stochastic Gradient Descent**"""

def stochastic_gradient_descent(loss_function, x, y, w):
  index_rand = np.random.permutation(len(y))[0]
  example_x = x[index_rand:index_rand+1,:]
  example_y = y[index_rand:index_rand+1,:]
  return - alpha * loss_function(example_x, example_y, w)

"""**Momentum Stochastic Gradient Descent**"""

def momentum_stochastic_gradient_descent(loss_function, x, y, w): 
  global prev_gradient
  val_ret = (gamma * prev_gradient) + batch_gradient_descent(loss_function, x, y, w)
  prev_gradient = val_ret
  return val_ret

"""**Adagrad Gradient Descent**"""

def adagrad_gradient_descent(loss_function, x, y, w):
    global diagGMatrix
    index_rand = np.random.permutation(len(y))[0]
    example_x = x[index_rand:index_rand+1,:]
    example_y = y[index_rand:index_rand+1,:]
    grad = loss_function(example_x, example_y, w)
    diagGMatrix = diagGMatrix + grad*grad
    val_ret = -alpha * grad / np.sqrt(diagGMatrix+e)
    return val_ret

"""**Adadelta Gradient Descent**"""

def adadelta_gradient_descent(loss_function, x, y, w):
  global exp_val_diag, exp_val_delta, delta
  grad = loss_function(x, y, w)
  exp_val_diag = gamma * exp_val_diag + (1 - gamma) * grad * grad
  exp_val_delta = gamma * exp_val_delta + (1 - gamma) * delta * delta
  delta = - np.sqrt(exp_val_delta + e) / np.sqrt(exp_val_diag + e) * grad
  return delta

"""**Adaptive Moment Estimation (Adam) Gradient Descent**"""

def adam_gradient_descent(loss_function, x, y, w):
  global first_moment, second_moment
  grad = loss_function(x, y, w)
  first_moment = beta1 * first_moment + (1 - beta1) * grad
  second_moment = beta2 * second_moment + (1 - beta2) * grad * grad 
  hat_first_moment = first_moment / (1 - beta1)
  hat_second_moment = second_moment / (1 - beta2)
  return - alpha * hat_first_moment / (np.sqrt(hat_second_moment) + e)

"""**Parameters**"""

w = np.zeros(shape = (4, 1))
alpha = 0.01  # learning rate
epochs = 10**6 #4
lambda_v = 0.00001 # tuning parameter, for regularization
prev_gradient = 0 # for momemtun stochastic gradient descent
gamma = 0.9 # for momemtun stochastic gradient descent
diagGMatrix = np.zeros(shape = (len(w), 1)) # accumulative matrix with w*w, for adagrad gradient descent
e = 1e-8 # smoothing term, for adagrad gradient descent
exp_val_diag = np.zeros(shape = (len(w), 1)) # for adadelta gradient descent
exp_val_delta = np.zeros(shape = (len(w), 1)) # for adadelta gradient descent
delta = np.zeros(shape = (len(w), 1)) # for adadelta gradient descent
first_moment = np.zeros(shape = (len(w), 1)) # for adan gradient descent
second_moment = np.zeros(shape = (len(w), 1)) # for adan gradient descent
beta1 = .9 # for adan gradient descent
beta2 = .999 # for adan gradient descent

import time

w = np.zeros(shape = (4, 1))

list_error = []
prev_gradient = 0 # for momemtun stochastic gradient descent
xcopy = rescaled(x_train)
exp_val_diag = np.zeros(shape = (len(w), 1)) #expected value of 

bef = time.time()
i = 0
while i <= epochs:
  i += 1
  #w = w + batch_gradient_descent(loss_function_mse, xcopy, y_train, w)
  #w = w + stochastic_gradient_descent(loss_function_mse, xcopy, y_train, w)
  #w = w + momentum_stochastic_gradient_descent(loss_function_mse, xcopy, y_train, w)
  w = w + adagrad_gradient_descent(loss_function_mse, xcopy, y_train, w)
  #w = w + adadelta_gradient_descent(loss_function_mse, xcopy, y_train, w)
  #w = w + adam_gradient_descent(loss_function_mse, xcopy, y_train, w)
  list_error.append(error_mse(xcopy, y_train, w))
  
  print(list_error[-1])
  #print(w)
  
aft = time.time()
print("Full time::",aft - bef)

"""$$w_0 + w_1 * x^1+ w_1 * x^2+ w_3 * x^3$$

**`Plotting Training DataSet`**
"""

yf_train = h(rescaled(x_train), w)
#yt.shape, x.shape
plt.scatter(x_train[:,1], y_train)
plt.plot(x_train[:,1], yf_train)

"""**`Plotting Test DataSet`**"""

yf_test = h(rescaled(x_test), w)
#yt.shape, x.shape
plt.scatter(x_test[:,1], y_test)
plt.plot(x_test[:,1], yf_test)

list_error = np.array(list_error)
#plt.plot(list_error[2124124:])
plt.plot(list_error)
plt.show()